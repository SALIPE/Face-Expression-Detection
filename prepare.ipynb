{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "number_of_labels = 7\n",
    "classes = ('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('expression_faces_dataset/images/train', transform=transform)\n",
    "validation_dataset = datasets.ImageFolder('expression_faces_dataset/images/validation', transform=transform)\n",
    "\n",
    "train_loaded = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loaded = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(train_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images in a test set is:  28832\n",
      "The number of batches per epoch is:  1802\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcPUlEQVR4nO3d245dxdku4MrGpu1ut/cGi2xAIYAi5SR3kuvKfSU5iJSDoASJQBJAYLxt98ZuYxLWyfpLHKz5vlOu7vBL63lOv645x6ZGf5rSWzV+8O233347AGCM8cPv+wAA+N9DUwBg0hQAmDQFACZNAYBJUwBg0hQAmDQFAKYfb/uHv/vd72L91q1br3wQR0dHsX54eBjre3t7G2vXrl2LY3/4w9wXW31l7I9+9KONtR/84AdxbFtz+O9///uVx7exrZ6OvV2Tdt7pmo0xxoULF155bKunY3v58mUc21y8eDHW03VL57xNfXd3N9aTnZ2dWL98+fIrj33ttddi/Ztvvon19H+jzbNWPz093Vhrz2Z7Btr4H/9487/tNofffffdWB/DLwUAvkNTAGDSFACYNAUAJk0BgElTAGDSFACYtl6ncOXKlVhP2duWu23Z2v39/Veut6xzk479PN9PtJqjXllf0dYhtHx4ui7nvU4hWVm7MUbOh6faGDnXvs13p7UG7bz+85//xHq6JyvrQpqVOTrG2nqA1XmYxrfr3aw++6v8UgBg0hQAmDQFACZNAYBJUwBg0hQAmLaOpKbtqZuVuN023522512NjaZ42Wr0bOXYzjOW1rY0bpHUFJFcvR8r21+vxl1XorYrUdoxxvj666831tr9aMeW5vHK2DHW5sJqPcWEV+PJqX7ekdT0+WcRk/dLAYBJUwBg0hQAmDQFACZNAYBJUwBg0hQAmLZep7C6zW1y8eLFWL906VKsp1xvy+22TPB5bo+98r2r6xTS569sjT3G2jbqq9f7PLdLTtfl5cuXrzx2jJ5tT+PbZ7c1Es+ePdtYa89eO++kPfdtO/KVtSGr6xTSZ6/O8fPeGrvxSwGASVMAYNIUAJg0BQAmTQGASVMAYNIUAJi2Xqdwnrnd1157LdZbnjllpVvmd2WP/ZYtb+edstItR92c5/qKlZz16lqBNn7lmq589snJSRx7enoa6yvrGNo8PDo6ivU0x69cuRLHtmuaPnt3dzeObWskmvN8T0RaQ9Hm8OqznZzFGge/FACYNAUAJk0BgElTAGDSFACYNAUAJk0BgGnrdQptT/aVPP/qvukr2dyVnPVqHjnl/ds1W5W++8KFC3HsixcvYj2d98oe+GP0NRIr162d99dff/1KtTH6OoT2XoKVzH2bh+m72/1oz266pnt7e3Fsq7f1S+nY2/VeeU/EyrsxthmfngHrFAA4U5oCAJOmAMCkKQAwaQoATJoCANOZRVJXtks+z21sWxyvRbhW4l8r8csWvTzPeovjtUjqeUYcV7Zhb5/d5lmKnbbzatvDN2kutWhmm6c7Ozsba+282nen424x3lZv9zOdd/u/0LY6T/OwzdEWSW3HluoiqQCcKU0BgElTAGDSFACYNAUAJk0BgElTAGDaep3CypbFLU/ctNxuykq342457HReq5nglOdvGe2Wo17Zyrndr7aOIV2X1bx+u+Zpq+bVLdjTZ7dtt589exbr7X6mz19d29HmStLWdqRr2ta7tOO6dOlSrKf7vbo2amWdwmrdOgUA/ms0BQAmTQGASVMAYNIUAJg0BQAmTQGAaW0BwXekfOzqWoGVffBX99BP59X2RW/Z84ODg421thagfffqNU/29vZiPeXLW55/df/+dj+Tds0uX768sdbW0rTMffvuk5OTjbV79+7FsY8ePXrlz27n1Z6vt956a2Ptvffei2Pbvd7d3Y31NNdanr/Nw3Q/22evritJ16X9X9iGXwoATJoCAJOmAMCkKQAwaQoATJoCANPWkdQWs0r1NrZFsFa0z24RrhTJW9meeowcqXv+/Hkcu7+/H+vtmu/s7GystchcO7b02S0y2q7ZStxvJa7aPrtp8eS2tfaXX365sfbZZ5/FsYeHh7GetqBux9Xirh9//PHG2v379+PYFgu9c+dOrKdt2lvUtkXCj4+PN9bacbc53OZpqrfz2oZfCgBMmgIAk6YAwKQpADBpCgBMmgIAk6YAwHRmW2f/8Ieb+8vqOoWVdQwt89vyyGktQtvat513+uzr16/HsS2PnDLaY+Tzbrn2Jl3zlcz8GHmete9u2jVdmYdtnrW8f8r0t7UEbX3FlStXNtbaWoB33nkn1tN2420L9idPnsT6V199FetXr17dWGv3uq2XSduNtzna6k06dusUADhTmgIAk6YAwKQpADBpCgBMmgIAk6YAwLT1OoWWdV5Zp9CcZ368uXDhwivVxujHna5py54/fPgw1tse+ykX37L+bQ3ErVu3NtZaNr1luNte9enY2lqBNk/T/Wxj2/svDg4OYv3nP//5xtovfvGLODa932KMvN6mHXda4zBGXm/TPjutcdhmfLpfbZ61/3fp2V+dZ+3/2er/08YvBQAmTQGASVMAYNIUAJg0BQAmTQGA6cwiqSkmdd4RqpWoYIuVpujaavTsk08+2Vj705/+FMc+ePAg1tO23GPkmOLdu3eXvvv4+Hhj7b333otjW1SwXfOVz25zYWWOtyhuivGOkY+9xXTbsaW50LYyb599dHS0sdail+1+tP9JLTqdtPNKcdk0/8fo87Bdl3Teq9tyj+GXAgDfoSkAMGkKAEyaAgCTpgDApCkAMGkKAExbr1No+deVtQirud201XM7rlZPufiWR/7jH/8Y63/5y1821tKWw2OM8atf/SrWX3/99VhPWr67ZddPTk421s5zHcIYObPf8vwr2ymvrOMZY4zbt2/Herqm33zzTRzb6mmb9rbeJW27PUZeA9G2ln/+/PlSfXd3d2Ot/U9px5bOe3XdiHUKAPyvoSkAMGkKAEyaAgCTpgDApCkAMGkKAExbr1No2dqVfOzqZ6d6yxu3HPbBwcHG2v379+PYllf+7W9/u7HW9t9vufg2PmXT2/04PDyM9TS+rXFoayTaeaf73e51++40z1peP+2/P0bO87d6y7WvZvKTlWe33cs2V9r409PTjbV2r9PapzHys93eA9E+e2XNV7sm2/BLAYBJUwBg0hQAmDQFACZNAYBJUwBg0hQAmM7sfQotk5+s5nbTsbW95Ft2Pe3vf/fu3Tj2rbfeivV03i1H3d47cHR0FOvpurT70TL5V65c2Vhr97Jl09s8fPHixcba6jVNef6WD79x40ast/u18ny1ZyC9d6DdrzYX0nVpY9NxjdGvSbpfbS1Bu2ZJu2bt+VqZ4+28tuGXAgCTpgDApCkAMGkKAEyaAgCTpgDAtHUkdSUS15znttztuFe2cm6xtVZP0bTj4+M4tm2H3M47xQHb/Wj1FAVsW0Q37bqk805Rvm2kediO6/bt27GeYrxjjPHBBx9srLU53KKdK1HbJl2z9ly3425zKd3vtl142+r8+fPnr3xcq/9Lz2J77MQvBQAmTQGASVMAYNIUAJg0BQAmTQGASVMAYDqzwGvLrn9fn7265Xf67rZWoB132ua2HffqWoJkZUviMfL6jJOTkzi2bV/drnm6pm2r5ra1djqv09PTOLZtjf2Tn/wk1p88ebKx9vHHH8ex7Zq/8cYbG2stE9+2nk/zsM2zdk3bNtHp2Ns8a8e2Ms9afXXr7VV+KQAwaQoATJoCAJOmAMCkKQAwaQoATJoCANP5bsz9f61m6ls2PWX6W954JfPbctLtvNKxrayf2Ea6pu2atPx42mv+6dOnr3xcY/S1BOm6rF6ztD6jXbNHjx7FelsP8P7772+stWvy17/+NdbTPWnHtfIulDbHnz17FuvtvFfeGdKe7fN8B8XK/8OzWC/mlwIAk6YAwKQpADBpCgBMmgIAk6YAwPRfiaSu+j63zj7Pz27jkxbdXNnWu22N3eKXaavm9tntml2+fDnWd3Z2NtbadsmtnqK2LYZ4fHwc648fP471v/3tbxtrV69ejWN3d3dj/eDgYGOtnVe7HysR4RcvXizVk9Vt1NNcac/eKpFUAP5rNAUAJk0BgElTAGDSFACYNAUAJk0BgGnrdQoruffVrbGblfErW+i2dQatnr67rQVYXaeQ6qvbjadtotM6gjF6frx998OHDzfWPvroozj21q1bsX7z5s2NtW+++SaOPTo6ivVPP/001q9cubKx1q5pW8eQ8v6Hh4dx7Mqz3e5lmwtta+2kzfGVrevbWpyV/6XNyqsA/odfCgBMmgIAk6YAwKQpADBpCgBMmgIAk6YAwLT1OoWWfz2Lfbw3Oc91CN/nOw/OIlP8qp+dcvXtvQItH56u2f7+fhzbrllaAzFGfpdDqo3Rzzvtsd/eO9A++/333z+3727vBrhz587G2oMHD+LY9k6DNBfas7kyz8bIz3a7Zq2ejv37XHd1Fu9y8EsBgElTAGDSFACYNAUAJk0BgElTAGDSFACYzmydwkqe/zzft7ByXGOs5ZHbvuqpvvq+hPbdaZ1C++yWH9/b29tYa+sUWj780qVLsZ7eefD666/HsSvXtL1PIb0PYYz83o4x8jsTVt+9kd5b0NY4NOm723E9f/481lfWhrT71c47rYFoa1JW1xKk++19CgCcKU0BgElTAGDSFACYNAUAJk0BgOnMIqkr0bPVbbfT+PbZ32ccdmX76nY/Vu5X22L666+/jvXbt29vrK3ejxTNHCNf03Y/2jVL20Sf59bxY6yd1+HhYayvPD/nGUVvseqV2Habwyvz7PT0NI5dPa+VaPQ2/FIAYNIUAJg0BQAmTQGASVMAYNIUAJg0BQCmrdcptO2Sr1+/vrG2up1r21Y4Zfpb5rflrFe2zl7dInfls1teOW3927YsbtsKp2vWjmt1S/A0F1bXfqRjT9tPj5HXOIwxxsHBQayna962kG7nle5XO+6VLb/bvU5zdIy+HiA926trJNJcaP8rV533mhi/FACYNAUAJk0BgElTAGDSFACYNAUAJk0BgGnrdQqPHz+O9d3d3Y21S5cuxbEr+fAxcp55ZR3CGDnb3o6rZZ3Tea++g6JlvFf2mm9Str3dj1Zvx5bu1+oe+il/3tYZtLx/y9yn+9mer729vVhPayzaGoiz2L9/kzbH2xqJ9Hytrr9Iz0+7Ju2afp/rn8bwSwGA79AUAJg0BQAmTQGASVMAYNIUAJi2jqTeuHEj1lNMqm1ZvLIV8xg5eta+eyXWtroN9HlGy1okNUXy2nm1a5buV4tetvt17969WP/nP/+5sdYiwi2mmCKpR0dHcWyLXbe5sL+/v7GWtq0fY4ybN2/G+p07d175s1uMN1mNVbd5mO53++x2v1K8+fj4OI5t26yvRNnPYlttvxQAmDQFACZNAYBJUwBg0hQAmDQFACZNAYBp63UKly9fzh8UtoNt2yG3XHzLDK9YWWuwur11ui6r59y+O2Wh29a+LeOdctotz//hhx/G+gcffBDrh4eHsZ60eZpy8S1b3q5Zu+ap3r675ebTebU1RG3b7qSd8+q29+n5bM9XWy+TXiXQXjPQ1imsbEdunQIAZ0pTAGDSFACYNAUAJk0BgElTAGDSFACYtl6nsLLWYDXPv2Jlz/WmXZOVPdvbNWs563ZsaXwb247tX//618ban//85zj24cOHsd5y8endAG1sO690P9P++mPkdzGsjm95/7bGaOV9JCtzZXWerdTbNWuffXJyEuvJ6rN7nuubxvBLAYDv0BQAmDQFACZNAYBJUwBg0hQAmDQFAKYze59C2ie/ZbD39vZivWWG01qDlvldWafQ1iG0etJy1G3P9ZUM+IsXL+LYL774Itb/8Ic/bKy14759+3ast73o03Vra1ZW3iPRrndbp9D24H/69OnGWnu+2vqMldx7++5UX53D5/melfYMpPUbbY62dVmr9VV+KQAwaQoATJoCAJOmAMCkKQAwaQoATFtHUj/66KNYT9GzFs18++23tz2M/6cU0fo+410rW/u2OF477pUYb4vpfvrpp7Ge4nwtctpiofv7+7Ge4pftmq1EVttxty2oWyz74OBgY+3Ro0dxbIqLj5G3G2/ndXp6GuspNtrmWXsGmvTd7X/S8fFxrL/22muvVGvHNcbas23rbADOlKYAwKQpADBpCgBMmgIAk6YAwKQpADBtvU7h97//fawfHh5urO3u7saxLbt+/fr1WD/PLHQaf55rBVqOemUNRPvuds1u3rwZ62ktwdWrV+PYtkX7ylqCNrZds3RP2tiW929z/Nq1axtrbVvuJ0+exPrOzs7GWtt2u513u+ZJewba85e25m7bdrdrunJe7ZqtbBl+Fuuu/FIAYNIUAJg0BQAmTQGASVMAYNIUAJg0BQCmrdcppD3Xx8h7tt+7dy+O/eSTT2L9N7/5TaynzPFKnrhpeeMm5bDbWoGWR27j0/svWqa+ZddTveXxW0b74sWLsZ6uS8u9t2u28v6LVm/vW7h169bGWlv7kd7FMEZeG5LWR4wxxvPnz2M93Y/V9w6svJcgzf8x8jtBxsj3q/1faM9ue/5W1jdtwy8FACZNAYBJUwBg0hQAmDQFACZNAYBp60jqT3/601hPsdAvvvgijv3HP/4R6++8806sp0hdixk2KdLaPnsl4thibS2O1yKO6X61425bnaeoYYvj3b17N9ZPTk5iPUU/23bJTbrmLSrbYoYt+pnGt/v19ttvx/qNGzc21tp5tWhnen5afLLN4ZVIeLtmLeabvrttu51eMzBGP+90Tdv92IZfCgBMmgIAk6YAwKQpADBpCgBMmgIAk6YAwLT1OoW2ze277767sfazn/0sjv3qq69iveV6U4a75eLbeaXM8Mo6hFZvWy03bcvwdM1apr5tf/3WW29trN2/fz+O3d/fj/V2v5LVLYtTZr9ttdzWdrQ5/ve//31jLa0zGKM/fymTn9YAjbG2ZXi7Zqv3K3338fFxHHvz5s1YT9elXZN2rz/77LNYT2t12rb22/BLAYBJUwBg0hQAmDQFACZNAYBJUwBg0hQAmLZep9D2Pk+53ZY9b5n6luu9cuXKxlrLMq9mpZPzXKfQ7kdbQ5HG7+3txbHt2H75y1++8ti2l/zp6Wmsp3cmtAx3mwtpr/o2z1ou/vnz57H+5ptvbqy98cYbcezOzk6sp+vS1oW093qk+726hqiNPzo6eqXaGH0tTrrf7dlsayCa9H6aNs+24ZcCAJOmAMCkKQAwaQoATJoCAJOmAMC0dSS1xdpSnK9FM9tnt0hqql+7di2OTRHGMXKkrkXPWlwvxUbbZ7do58q2w+24mxTn+/Wvfx3HPn36NNYfPnwY6+m6rEYc01xpseoWd23Hdvfu3Y21FMkeo8+VFElt87Cdd7qmK8/HNt+9Mhfa/4X0/LT/d+28U/x4jPx8PXnyJI7dhl8KAEyaAgCTpgDApCkAMGkKAEyaAgCTpgDAtPU6hZbrXdkGumWC2zqGr776amOtbWncMt5pK+eWN17JYbexLT/e6ru7uxtrLVN/8eLFWE/n1ca2bbvb/UzzsM2zdmzpmrbrvboFdTr2NradV8r7t8z9ylqdti6krUNY+Z+U5v8Y/djS/4V23E3aon2MfM1ff/31pe8ewy8FAL5DUwBg0hQAmDQFACZNAYBJUwBg0hQAmLZep7C/vx/rKQvd8uEtF3/58uVYT+9TaPuLtwx3WmOxum960q5ZyzK3DHfKOq/mrFPGu+W/23G3+vPnzzfW2jxq+/en+93GHh8fx3pby5Ny9W0Ot3U+ae3H6elpHLuynqYdV7vX7Zol7flp9zONb2sg2jte2lqcdM3bXNiGXwoATJoCAJOmAMCkKQAwaQoATJoCAJOmAMC09TqFlex6W4fQ8sopez5G3kP8yy+/jGOfPn0a62l9Rlun0KTMfvvslrNuGe703e1et3rKeLe5sJofX7kn7d0A6Zq2427Z8+vXr8d6es9EO+eV9QCr55WuaZsLJycnsd6u2d27dzfWPv/88zi2/c9J87CtMWpzuK2nuXr16sZam8Pb8EsBgElTAGDSFACYNAUAJk0BgElTAGDaOpJ648aNWD86OtpYazGptv1ui3iluN6bb74Zxz548CDWU9zv0qVLcexKxLFdkxYLffbsWawnq1tMp5ji6jbPK1HBlZjuGGtbobfo5krku82zVk/bY7fr3ba3fvTo0cZaO+d2v9q23um72/PRvnslnty+u/2/S1bm6P/wSwGASVMAYNIUAJg0BQAmTQGASVMAYNIUAJi2XqfQ8uNpnULLf7dMcMtCpwz4Sv57jDEODg421lbXEqTzatesfXYbn/L8bR1Cu19pS+R2XG3tR1vnkLLrqxnuly9fbqy142prBdo1T9n39nysfHY7r7b9ddrmeXWON48fP95Ya3O4nXdaG7W7uxvHrq5vSusY0hzdll8KAEyaAgCTpgDApCkAMGkKAEyaAgCTpgDAtPU6hfv378d6yiu3XG7LUad3GrR6yxvv7+/HesqAP336NI49PDyM9bS+YjWv38anrPPJyUkc26T70dYKrM6FNL7lv9u7A1Juvh1Xy/OvzPHVa5rqba1AW790586djbX2XoGVd2eMkZ+R9ny0/wvt/RhJe19Cmwtp/JMnT+LY999/P9bH8EsBgO/QFACYNAUAJk0BgElTAGDSFACYts5VtZjU5cuXN9Za/Ov4+DjWW/wyxeLStsBjrG3f2+J67bhTpLVFgB89ehTrN27ciPWbN29urKXtjsfI21OPkeOXLT7Z6i1Wmsa3qG3bTjmNb8e1ur11muMtFto+O0U/V2K6Y4xx7969WE9W4+Tpf1I77jYPU5x2Ja46Rr9f6X9W+z+9Db8UAJg0BQAmTQGASVMAYNIUAJg0BQAmTQGAaetAbcvz7+3tbay1HHXLvbetZlumOElZ5jHysbVM8LVr12I9rQdomfovv/xyqX5wcLCxdvfu3Ti2Ze5Tvd3rlot/+fJlrCdtLUE7tpQ/b7n3tlanXdOVrbPbupP0fLVrlp77MfLz1dYhtOer/U9K16Vt272yXmZlnUH77DHyPG1zeBt+KQAwaQoATJoCAJOmAMCkKQAwaQoATJoCANOZrVNI+fKWCW7Z85bbTce2mhlOOeuWLW9rDdJ1afnvd955Z+m707scPv/88zi2re3Y3d3dWGt5/StXrsR6ez9GepdDm0fpuMfIufm2h37L3K+866Hl/ds6n1u3br3S947Rr2l650F7Nts1adI1b8/9yhqIdD238fDhw1hPc609P9vwSwGASVMAYNIUAJg0BQAmTQGASVMAYNIUAJi2XqfQ8sopc7yyFmCMtT3CV/b+HyPnkds7C9r6jJQ3btny1f3g0zVvawnSuxjGGOPRo0cba20erc6VlNlv72o4Pj6O9SStjxijr1No+fKU929zOI0dI8+ltoaonXeax6tzoY1Pz0BbI9GuaVov0+ZRO692bG1dyiq/FACYNAUAJk0BgElTAGDSFACYNAUApjPbOjvZ2dmJ9RaZe/z4cayn2FyLd6UtpMcY48mTJxtrLeJ49erVWE+xt8PDwzi2ad+d4rAtZtjirhcuXNhYaxHHdt5trqQ47YcffhjHtmNL96tFStv9SNdsjPz8tefrzp07sf7gwYONtRY/btHNle2r2zxrMd+0/XzbJj3FqsfI5310dBTHtu3GW9T2/v37G2tty/1t+KUAwKQpADBpCgBMmgIAk6YAwKQpADBpCgBMP/i2hX0B+P+GXwoATJoCAJOmAMCkKQAwaQoATJoCAJOmAMCkKQAwaQoATP8HkaSv8+HPCx0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(images[5], normalize=False)\n",
    "print(\"The number of images in a test set is: \", len(train_loaded)*16)\n",
    "\n",
    "print(\"The number of batches per epoch is: \", len(train_loaded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CNN (Class of Neural Network) with the following 14 layers:\n",
    "\n",
    "###### Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> ReLU -> MaxPool -> Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> ReLU -> Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=9, kernel_size=6, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(9)\n",
    "        self.conv2 = nn.Conv2d(in_channels=9, out_channels=9, kernel_size=6, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(9)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=9, out_channels=18, kernel_size=6, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(18)\n",
    "        self.conv5 = nn.Conv2d(in_channels=18, out_channels=18, kernel_size=6, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(18)\n",
    "        self.fc1 = nn.Linear(3*48*48, 48)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool(output)                        \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = F.relu(self.bn5(self.conv5(output)))     \n",
    "        output = torch.flatten(input, 1)\n",
    "        output = self.fc1(output)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a neural network model \n",
    "model = ConvolutionNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    path = \"./myFirstModel.pth\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def testAccuracy():\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validation_loaded:\n",
    "            images, labels = data\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)\n",
    "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "def train(num_epochs):\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Define your execution device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "    # Convert model parameters and buffers to CPU or Cuda\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loaded, 0):\n",
    "            \n",
    "            # get the inputs\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict classes using images from the training set\n",
    "            outputs = model(images)\n",
    "            # compute the loss based on model output and real labels\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            # adjust parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Let's print statistics for every 1,000 images\n",
    "            running_loss += loss.item()     # extract the loss value\n",
    "            if i % 1000 == 999:    \n",
    "                # print every 1000 (twice per epoch) \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 1000))\n",
    "                # zero the loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Compute and print the average accuracy fo this epoch when tested over all 10000 test images\n",
    "        accuracy = testAccuracy()\n",
    "        print('For epoch', epoch+1,'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
    "        \n",
    "        # we want to save the model if the accuracy is the best\n",
    "        if accuracy > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show the images\n",
    "def imageshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to test the model with a batch of images and show the labels predictions\n",
    "def testBatch():\n",
    "    # get batch of images from the test DataLoader  \n",
    "    images, labels = next(iter(validation_loaded))\n",
    "\n",
    "    # show all images as one image grid\n",
    "    imageshow(torchvision.utils.make_grid(images))\n",
    "   \n",
    "    # Show the real labels on the screen \n",
    "    print('Real labels: ', ' '.join('%5s' % classes[labels[j]] \n",
    "                               for j in range(batch_size)))\n",
    "  \n",
    "    # Let's see what if the model identifiers the  labels of those example\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # We got the probability for every 10 labels. The highest (max) probability should be correct label\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Let's show the predicted labels on the screen to compare with the real ones\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] \n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cpu device\n",
      "[1,  1000] loss: 1.982\n",
      "For epoch 1 the test accuracy over the whole test set is 23 %\n",
      "[2,  1000] loss: 2.019\n",
      "For epoch 2 the test accuracy over the whole test set is 24 %\n",
      "[3,  1000] loss: 2.009\n",
      "For epoch 3 the test accuracy over the whole test set is 28 %\n",
      "[4,  1000] loss: 1.928\n",
      "For epoch 4 the test accuracy over the whole test set is 27 %\n",
      "[5,  1000] loss: 1.968\n",
      "For epoch 5 the test accuracy over the whole test set is 34 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Let's build our model\n",
    "train(5)\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
