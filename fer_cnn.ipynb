{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment prepare (imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler,ConcatDataset,Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "Para abordagem de será feito uma validação cruzada apenas para a validação do treinamento da **epoch**, onde usaremos dois datasets:\n",
    "\n",
    "- Fer Affectnet Database: https://www.kaggle.com/datasets/noamsegal/affectnet-training-data?select=contempt\n",
    "- Corrective re-annotation of FER - CK+ - KDEF: https://www.kaggle.com/datasets/sudarshanvaidya/corrective-reannotation-of-fer-ck-kdef?select=fer_ckplus_kdef\n",
    "- Dataset montado a partir de outros datasets:\n",
    "    1. FER Dataset: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "    2. CK Plus Dataset: https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch/tree/master/CK%2B48\n",
    "    3. KDEF Dataset: https://www.kdef.se/download-2/register.html\n",
    "\n",
    "Esses datasets possuem uma oitava classificação de expressão facial, na qual não usaremos em nosso desenvolvimento e aplicação, que é o **contempt**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset dispositions"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation functions and hyperparams\n",
    "\n",
    "Aqui optaremos por duas transformações de imagens de treinamento, em modelos como AlexNet, VGG que trabalham com imagens de escala maior usaremos *227x227 pixels* para treinamento do AlexNet e *224x224 pixels* para o VGG.\n",
    "\n",
    "Outra abordagem será utilizar imagens de menor escala, para um rede neural menor de desenvolvimento próprio, baseado em outros notebooks e estudos relacionados, para esse modelo será utilizado amostras de imagens com a escala de *64x64 pixels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "classes = ('angry', 'fear', 'happy', 'neutral', 'sad', 'surprise','disgust')\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.Resize((224,224)),\n",
    "    # transforms.Resize((227,227)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder('./color_dataset_2/train', transform=train_transform)\n",
    "# train_loaded = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder('./fer_ckplus_dataset', transform=train_transform)\n",
    "concat_data = ConcatDataset([train_dataset,validation_dataset])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation \n",
    "\n",
    "###### We'll use the cross validation with a diferent dataset, to improve the cnn knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet model to 64x64px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNeuralNetwork(nn.Module):\n",
    "    def __init__(self,num_classes=7):\n",
    "        super(ConvolutionNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.conv4 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.drop1=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(96)\n",
    "        self.conv6 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=4, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(96*6*6, 192)\n",
    "        self.drop2=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(192, 96)\n",
    "        self.fc3 = nn.Linear(96, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool1(output)  \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))      \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = self.pool2(output)      \n",
    "        output = self.drop1(output)  \n",
    "\n",
    "        output = F.relu(self.bn5(self.conv5(output)))      \n",
    "        output = F.relu(self.bn6(self.conv6(output)))  \n",
    "        output = self.pool3(output)  \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = self.drop2(output)\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    #acuracia no teste=62.52% || 63.08%\n",
    "    #acuracia no treino=81.46% || 86.80%\n",
    "    def __init__(self,num_classes=7):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=10, stride=4, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.pool1= nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))   \n",
    "        output = self.pool1(output)     \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool2(output)    \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))     \n",
    "        output = F.relu(self.bn4(self.conv4(output)))   \n",
    "        output = F.relu(self.bn5(self.conv5(output))) \n",
    "        output = self.pool3(output)   \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionNeuralNetwork()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing on: cuda\n"
     ]
    }
   ],
   "source": [
    "# accelerator = Accelerator()\n",
    "# device = accelerator.device\n",
    "# Define your execution device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Runing on: \"+ (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# model = torch.nn.Transformer().to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate,  weight_decay = 0.001)\n",
    "\n",
    "# model, optimizer, data = accelerator.prepare(model, optimizer,concat_data )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), \"apurated_model_mycnn.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        # accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "\n",
    "            images,labels = images.to(device),labels.to(device)\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            \n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(concat_data)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_samples =  SubsetRandomSampler(train_idx)\n",
    "\n",
    "        test_loader = DataLoader(concat_data, batch_size=batch_size, sampler=test_sampler)\n",
    "        train_loader = DataLoader(concat_data, batch_size=batch_size, sampler=train_samples)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_correct=train_epoch(model,device,train_loader)\n",
    "            test_loss, test_correct=valid_epoch(model,device,test_loader)\n",
    "\n",
    "            train_loss = train_loss / len(train_loader.sampler)\n",
    "            train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "\n",
    "            test_loss = test_loss / len(test_loader.sampler)\n",
    "            test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "            print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                                    num_epochs,\n",
    "                                                                                                                    train_loss,\n",
    "                                                                                                                    test_loss,\n",
    "                                                                                                                    train_acc,\n",
    "                                                                                                                    test_acc))\n",
    "            if train_acc > best_accuracy:\n",
    "                saveModel()\n",
    "                best_accuracy = train_acc\n",
    "                print(\"Best Accuracy:{} %\".format(best_accuracy))\n",
    "\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)   \n",
    "\n",
    "    df_history = pd.DataFrame(data=history)\n",
    "    df_history.to_csv(\"historic_mycnn.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 274350 KiB | 698368 KiB |   2603 MiB |   2336 MiB |\\n|       from large pool | 236548 KiB | 622592 KiB |   2291 MiB |   2060 MiB |\\n|       from small pool |  37802 KiB |  75776 KiB |    312 MiB |    275 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 274350 KiB | 698368 KiB |   2603 MiB |   2336 MiB |\\n|       from large pool | 236548 KiB | 622592 KiB |   2291 MiB |   2060 MiB |\\n|       from small pool |  37802 KiB |  75776 KiB |    312 MiB |    275 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 270321 KiB | 690176 KiB |   2555 MiB |   2291 MiB |\\n|       from large pool | 232561 KiB | 614400 KiB |   2243 MiB |   2016 MiB |\\n|       from small pool |  37760 KiB |  75776 KiB |    312 MiB |    275 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 292864 KiB | 733184 KiB |   1088 MiB |    802 MiB |\\n|       from large pool | 251904 KiB | 655360 KiB |    966 MiB |    720 MiB |\\n|       from small pool |  40960 KiB |  77824 KiB |    122 MiB |     82 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  18514 KiB | 110509 KiB |   3490 MiB |   3472 MiB |\\n|       from large pool |  15356 KiB |  88664 KiB |   3178 MiB |   3163 MiB |\\n|       from small pool |   3158 KiB |  21845 KiB |    311 MiB |    308 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     380    |     738    |    3327    |    2947    |\\n|       from large pool |      72    |     168    |     663    |     591    |\\n|       from small pool |     308    |     570    |    2664    |    2356    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     380    |     738    |    3327    |    2947    |\\n|       from large pool |      72    |     168    |     663    |     591    |\\n|       from small pool |     308    |     570    |    2664    |    2356    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      32    |      70    |     109    |      77    |\\n|       from large pool |      12    |      32    |      48    |      36    |\\n|       from small pool |      20    |      38    |      61    |      41    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      19    |      46    |     881    |     862    |\\n|       from large pool |       6    |      17    |     268    |     262    |\\n|       from small pool |      13    |      38    |     613    |     600    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[383], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(\u001b[39m20\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[381], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(concat_data, batch_size\u001b[39m=\u001b[39mbatch_size, sampler\u001b[39m=\u001b[39mtrain_samples)\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 18\u001b[0m     train_loss, train_correct\u001b[39m=\u001b[39mtrain_epoch(model,device,train_loader)\n\u001b[0;32m     19\u001b[0m     test_loss, test_correct\u001b[39m=\u001b[39mvalid_epoch(model,device,test_loader)\n\u001b[0;32m     21\u001b[0m     train_loss \u001b[39m=\u001b[39m train_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39msampler)\n",
      "Cell \u001b[1;32mIn[380], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, device, dataloader)\u001b[0m\n\u001b[0;32m      2\u001b[0m train_loss,train_correct\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m,\u001b[39m0\u001b[39m\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m      6\u001b[0m     images,labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device),labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\dataset.py:243\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\datasets\\folder.py:246\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[0;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[0;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(20)\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((227,227)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder('./color_dataset_2/test', transform=test_transform)\n",
    "test_loaded = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loaded:\n",
    "\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBatch(model):\n",
    "    model.eval()\n",
    "    # get batch of images from the test DataLoader  \n",
    "    images, labels = next(iter(test_loaded))\n",
    "   \n",
    "    # show all images as one image grid\n",
    "    img = torchvision.utils.make_grid(images)     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "   \n",
    "    # Show the real labels on the screen \n",
    "    print('Real labels: ', ' '.join('%5s' % classes[labels[j]] \n",
    "                               for j in range(batch_size)))\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        \n",
    "        # Let's show the predicted labels on the screen to compare with the real ones\n",
    "        print('Predicted: ', ' '.join('%5s' % classes[predictions[j]] \n",
    "                                for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "path = \"apurated_model_alex.pth\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG Test Loss:1.431 AVG Test Acc 57.25 %\n"
     ]
    }
   ],
   "source": [
    "valid_loss,val_correct = test_model(model)\n",
    "\n",
    "valid_loss = valid_loss / len(test_loaded.sampler)\n",
    "val_acc = val_correct / len(test_loaded.sampler) * 100\n",
    "\n",
    "print(\"AVG Test Loss:{:.3f} AVG Test Acc {:.2f} %\".format(valid_loss,val_acc))\n",
    "                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
