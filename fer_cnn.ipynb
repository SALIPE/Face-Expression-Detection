{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment prepare (imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler,ConcatDataset,Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "Para abordagem de será feito uma validação cruzada apenas para a validação do treinamento da **epoch**, onde usaremos dois datasets:\n",
    "\n",
    "- Fer Affectnet Database: https://www.kaggle.com/datasets/noamsegal/affectnet-training-data?select=contempt\n",
    "- Corrective re-annotation of FER - CK+ - KDEF: https://www.kaggle.com/datasets/sudarshanvaidya/corrective-reannotation-of-fer-ck-kdef?select=fer_ckplus_kdef\n",
    "- Dataset montado a partir de outros datasets:\n",
    "    1. FER Dataset: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "    2. CK Plus Dataset: https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch/tree/master/CK%2B48\n",
    "    3. KDEF Dataset: https://www.kdef.se/download-2/register.html\n",
    "\n",
    "Esses datasets possuem uma oitava classificação de expressão facial, na qual não usaremos em nosso desenvolvimento e aplicação, que é o **contempt**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset dispositions"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation functions and hyperparams\n",
    "\n",
    "Aqui optaremos por duas transformações de imagens de treinamento, em modelos como AlexNet, VGG que trabalham com imagens de escala maior usaremos *227x227 pixels* para treinamento do AlexNet e *224x224 pixels* para o VGG.\n",
    "\n",
    "Outra abordagem será utilizar imagens de menor escala, para um rede neural menor de desenvolvimento próprio, baseado em outros notebooks e estudos relacionados, para esse modelo será utilizado amostras de imagens com a escala de *64x64 pixels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "classes = ('angry', 'fear', 'happy', 'neutral', 'sad', 'surprise','disgust')\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((64,64)),\n",
    "    # transforms.Resize((224,224)),\n",
    "    transforms.Resize((227,227)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder('./color_dataset_2/train', transform=train_transform)\n",
    "# train_loaded = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder('./fer_ckplus_dataset', transform=train_transform)\n",
    "concat_data = ConcatDataset([train_dataset,validation_dataset])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation \n",
    "\n",
    "###### We'll use the cross validation with a diferent dataset, to improve the cnn knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet model to 64x64px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialExpressionAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(FacialExpressionAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    #acuracia no teste=62.52% || 63.08%\n",
    "    #acuracia no treino=81.46% || 86.80%\n",
    "    def __init__(self,num_classes=7):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=10, stride=4, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.pool1= nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(384)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(3,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))   \n",
    "        output = self.pool1(output)     \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool2(output)    \n",
    "\n",
    "        output = F.relu(self.bn3(self.conv3(output)))     \n",
    "        output = F.relu(self.bn4(self.conv4(output)))   \n",
    "        output = F.relu(self.bn5(self.conv5(output))) \n",
    "        output = self.pool3(output)   \n",
    "\n",
    "        output = output.reshape(output.size(0),-1)\n",
    "\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define your execution device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Runing on: \"+ (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate,  weight_decay = 0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), \"apurated_model_alex.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "\n",
    "            images,labels = images.to(device),labels.to(device)\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(concat_data)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_samples =  SubsetRandomSampler(train_idx)\n",
    "\n",
    "        test_loader = DataLoader(concat_data, batch_size=batch_size, sampler=test_sampler)\n",
    "        train_loader = DataLoader(concat_data, batch_size=batch_size, sampler=train_samples)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_correct=train_epoch(model,device,train_loader)\n",
    "            test_loss, test_correct=valid_epoch(model,device,test_loader)\n",
    "\n",
    "            train_loss = train_loss / len(train_loader.sampler)\n",
    "            train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "\n",
    "            test_loss = test_loss / len(test_loader.sampler)\n",
    "            test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "            print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                                    num_epochs,\n",
    "                                                                                                                    train_loss,\n",
    "                                                                                                                    test_loss,\n",
    "                                                                                                                    train_acc,\n",
    "                                                                                                                    test_acc))\n",
    "            if train_acc > best_accuracy:\n",
    "                saveModel()\n",
    "                best_accuracy = train_acc\n",
    "                print(\"Best Accuracy:{} %\".format(best_accuracy))\n",
    "\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)   \n",
    "\n",
    "    df_history = pd.DataFrame(data=history)\n",
    "    df_history.to_csv(\"historic_alex.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 478346 KiB |   2039 MiB | 269051 GiB | 269050 GiB |\\n|       from large pool | 477783 KiB |   2037 MiB | 268271 GiB | 268270 GiB |\\n|       from small pool |    563 KiB |      8 MiB |    780 GiB |    780 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 478346 KiB |   2039 MiB | 269051 GiB | 269050 GiB |\\n|       from large pool | 477783 KiB |   2037 MiB | 268271 GiB | 268270 GiB |\\n|       from small pool |    563 KiB |      8 MiB |    780 GiB |    780 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 478211 KiB |   2035 MiB | 267312 GiB | 267311 GiB |\\n|       from large pool | 477654 KiB |   2033 MiB | 266533 GiB | 266533 GiB |\\n|       from small pool |    556 KiB |      8 MiB |    778 GiB |    778 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 534528 KiB |   2162 MiB |   4788 MiB |   4266 MiB |\\n|       from large pool | 532480 KiB |   2154 MiB |   4764 MiB |   4244 MiB |\\n|       from small pool |   2048 KiB |     10 MiB |     24 MiB |     22 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  56182 KiB | 168245 KiB |  82116 GiB |  82116 GiB |\\n|       from large pool |  54697 KiB | 166955 KiB |  80960 GiB |  80960 GiB |\\n|       from small pool |   1485 KiB |   5181 KiB |   1156 GiB |   1156 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      75    |     284    |   37097 K  |   37097 K  |\\n|       from large pool |      15    |      59    |   14228 K  |   14228 K  |\\n|       from small pool |      60    |     226    |   22868 K  |   22868 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      75    |     284    |   37097 K  |   37097 K  |\\n|       from large pool |      15    |      59    |   14228 K  |   14228 K  |\\n|       from small pool |      60    |     226    |   22868 K  |   22868 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       8    |      31    |      68    |      60    |\\n|       from large pool |       7    |      27    |      56    |      49    |\\n|       from small pool |       1    |       5    |      12    |      11    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      16    |      28    |   14908 K  |   14908 K  |\\n|       from large pool |       6    |      14    |    7281 K  |    7281 K  |\\n|       from small pool |      10    |      16    |    7627 K  |    7627 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/20 AVG Training Loss:1.704 AVG Test Loss:1.326 AVG Training Acc 32.15 % AVG Test Acc 48.02 %\n",
      "Best Accuracy:32.14722052535125 %\n",
      "Epoch:2/20 AVG Training Loss:1.216 AVG Test Loss:1.213 AVG Training Acc 52.15 % AVG Test Acc 52.95 %\n",
      "Best Accuracy:52.15142028100183 %\n",
      "Epoch:3/20 AVG Training Loss:1.124 AVG Test Loss:1.121 AVG Training Acc 55.83 % AVG Test Acc 56.61 %\n",
      "Best Accuracy:55.83384239462431 %\n",
      "Epoch:4/20 AVG Training Loss:1.065 AVG Test Loss:1.060 AVG Training Acc 58.89 % AVG Test Acc 59.03 %\n",
      "Best Accuracy:58.893937080024436 %\n",
      "Epoch:5/20 AVG Training Loss:1.035 AVG Test Loss:1.018 AVG Training Acc 60.28 % AVG Test Acc 61.02 %\n",
      "Best Accuracy:60.27985644471594 %\n",
      "Epoch:6/20 AVG Training Loss:1.011 AVG Test Loss:0.996 AVG Training Acc 61.25 % AVG Test Acc 61.71 %\n",
      "Best Accuracy:61.25152718387293 %\n",
      "Epoch:7/20 AVG Training Loss:0.996 AVG Test Loss:1.028 AVG Training Acc 62.09 % AVG Test Acc 60.20 %\n",
      "Best Accuracy:62.09338729383018 %\n",
      "Epoch:8/20 AVG Training Loss:0.985 AVG Test Loss:0.989 AVG Training Acc 62.53 % AVG Test Acc 62.05 %\n",
      "Best Accuracy:62.53436163714111 %\n",
      "Epoch:9/20 AVG Training Loss:0.971 AVG Test Loss:0.989 AVG Training Acc 63.13 % AVG Test Acc 62.55 %\n",
      "Best Accuracy:63.12614538790471 %\n",
      "Epoch:10/20 AVG Training Loss:0.963 AVG Test Loss:0.984 AVG Training Acc 63.33 % AVG Test Acc 63.37 %\n",
      "Best Accuracy:63.33422419059255 %\n",
      "Epoch:11/20 AVG Training Loss:0.958 AVG Test Loss:0.956 AVG Training Acc 63.63 % AVG Test Acc 63.60 %\n",
      "Best Accuracy:63.626298106292 %\n",
      "Epoch:12/20 AVG Training Loss:0.949 AVG Test Loss:0.989 AVG Training Acc 63.92 % AVG Test Acc 62.34 %\n",
      "Best Accuracy:63.922189981673796 %\n",
      "Epoch:13/20 AVG Training Loss:0.947 AVG Test Loss:1.028 AVG Training Acc 63.93 % AVG Test Acc 61.72 %\n",
      "Best Accuracy:63.92600794135615 %\n",
      "Epoch:14/20 AVG Training Loss:0.941 AVG Test Loss:1.184 AVG Training Acc 64.16 % AVG Test Acc 56.66 %\n",
      "Best Accuracy:64.15699450213805 %\n",
      "Epoch:15/20 AVG Training Loss:0.939 AVG Test Loss:0.966 AVG Training Acc 64.11 % AVG Test Acc 63.77 %\n",
      "Epoch:16/20 AVG Training Loss:0.937 AVG Test Loss:0.973 AVG Training Acc 64.55 % AVG Test Acc 62.88 %\n",
      "Best Accuracy:64.55406230910201 %\n",
      "Epoch:17/20 AVG Training Loss:0.933 AVG Test Loss:1.033 AVG Training Acc 64.54 % AVG Test Acc 60.11 %\n",
      "Epoch:18/20 AVG Training Loss:0.932 AVG Test Loss:0.961 AVG Training Acc 64.58 % AVG Test Acc 63.31 %\n",
      "Best Accuracy:64.5769700671961 %\n",
      "Epoch:19/20 AVG Training Loss:0.931 AVG Test Loss:0.944 AVG Training Acc 64.60 % AVG Test Acc 64.15 %\n",
      "Best Accuracy:64.60369578497252 %\n",
      "Epoch:20/20 AVG Training Loss:0.929 AVG Test Loss:0.952 AVG Training Acc 64.81 % AVG Test Acc 63.91 %\n",
      "Best Accuracy:64.80986560781918 %\n",
      "Fold 2\n",
      "Epoch:1/20 AVG Training Loss:0.934 AVG Test Loss:0.967 AVG Training Acc 64.49 % AVG Test Acc 63.65 %\n",
      "Epoch:2/20 AVG Training Loss:0.928 AVG Test Loss:0.938 AVG Training Acc 64.83 % AVG Test Acc 63.92 %\n",
      "Best Accuracy:64.83468234575443 %\n",
      "Epoch:3/20 AVG Training Loss:0.931 AVG Test Loss:0.929 AVG Training Acc 64.70 % AVG Test Acc 64.63 %\n",
      "Epoch:4/20 AVG Training Loss:0.927 AVG Test Loss:0.967 AVG Training Acc 64.93 % AVG Test Acc 63.48 %\n",
      "Best Accuracy:64.93204031765426 %\n",
      "Epoch:5/20 AVG Training Loss:0.925 AVG Test Loss:0.954 AVG Training Acc 64.83 % AVG Test Acc 63.55 %\n",
      "Epoch:6/20 AVG Training Loss:0.922 AVG Test Loss:0.926 AVG Training Acc 64.99 % AVG Test Acc 65.01 %\n",
      "Best Accuracy:64.9912186927306 %\n",
      "Epoch:7/20 AVG Training Loss:0.918 AVG Test Loss:0.967 AVG Training Acc 65.35 % AVG Test Acc 63.55 %\n",
      "Best Accuracy:65.3501069028711 %\n",
      "Epoch:8/20 AVG Training Loss:0.924 AVG Test Loss:0.981 AVG Training Acc 64.99 % AVG Test Acc 63.10 %\n",
      "Epoch:9/20 AVG Training Loss:0.920 AVG Test Loss:1.078 AVG Training Acc 65.08 % AVG Test Acc 58.50 %\n",
      "Epoch:10/20 AVG Training Loss:0.917 AVG Test Loss:0.978 AVG Training Acc 65.24 % AVG Test Acc 62.86 %\n",
      "Epoch:11/20 AVG Training Loss:0.921 AVG Test Loss:0.911 AVG Training Acc 65.08 % AVG Test Acc 65.64 %\n",
      "Epoch:12/20 AVG Training Loss:0.917 AVG Test Loss:0.928 AVG Training Acc 65.09 % AVG Test Acc 64.70 %\n",
      "Epoch:13/20 AVG Training Loss:0.913 AVG Test Loss:1.024 AVG Training Acc 65.21 % AVG Test Acc 60.38 %\n",
      "Epoch:14/20 AVG Training Loss:0.915 AVG Test Loss:0.990 AVG Training Acc 65.30 % AVG Test Acc 62.12 %\n",
      "Epoch:15/20 AVG Training Loss:0.915 AVG Test Loss:0.974 AVG Training Acc 65.30 % AVG Test Acc 62.94 %\n",
      "Epoch:16/20 AVG Training Loss:0.913 AVG Test Loss:1.051 AVG Training Acc 65.32 % AVG Test Acc 59.66 %\n",
      "Epoch:17/20 AVG Training Loss:0.913 AVG Test Loss:0.958 AVG Training Acc 65.29 % AVG Test Acc 62.79 %\n",
      "Epoch:18/20 AVG Training Loss:0.914 AVG Test Loss:0.945 AVG Training Acc 65.15 % AVG Test Acc 64.22 %\n",
      "Epoch:19/20 AVG Training Loss:0.913 AVG Test Loss:0.992 AVG Training Acc 65.32 % AVG Test Acc 62.17 %\n",
      "Epoch:20/20 AVG Training Loss:0.912 AVG Test Loss:0.918 AVG Training Acc 65.35 % AVG Test Acc 64.71 %\n",
      "Best Accuracy:65.35392486255344 %\n",
      "Fold 3\n",
      "Epoch:1/20 AVG Training Loss:0.917 AVG Test Loss:0.960 AVG Training Acc 64.99 % AVG Test Acc 63.56 %\n",
      "Epoch:2/20 AVG Training Loss:0.912 AVG Test Loss:0.937 AVG Training Acc 65.43 % AVG Test Acc 64.58 %\n",
      "Best Accuracy:65.42646609651803 %\n",
      "Epoch:3/20 AVG Training Loss:0.912 AVG Test Loss:0.966 AVG Training Acc 65.50 % AVG Test Acc 63.72 %\n",
      "Best Accuracy:65.4990073304826 %\n",
      "Epoch:4/20 AVG Training Loss:0.913 AVG Test Loss:0.940 AVG Training Acc 65.37 % AVG Test Acc 64.58 %\n",
      "Epoch:5/20 AVG Training Loss:0.912 AVG Test Loss:0.905 AVG Training Acc 65.50 % AVG Test Acc 65.76 %\n",
      "Best Accuracy:65.50282529016494 %\n",
      "Epoch:6/20 AVG Training Loss:0.912 AVG Test Loss:1.016 AVG Training Acc 65.37 % AVG Test Acc 61.69 %\n",
      "Epoch:7/20 AVG Training Loss:0.907 AVG Test Loss:0.934 AVG Training Acc 65.52 % AVG Test Acc 64.92 %\n",
      "Best Accuracy:65.51618814905315 %\n",
      "Epoch:8/20 AVG Training Loss:0.907 AVG Test Loss:0.943 AVG Training Acc 65.56 % AVG Test Acc 64.11 %\n",
      "Best Accuracy:65.55818570555894 %\n",
      "Epoch:9/20 AVG Training Loss:0.904 AVG Test Loss:0.942 AVG Training Acc 65.84 % AVG Test Acc 65.01 %\n",
      "Best Accuracy:65.84262370189371 %\n",
      "Epoch:10/20 AVG Training Loss:0.907 AVG Test Loss:0.939 AVG Training Acc 65.84 % AVG Test Acc 64.42 %\n",
      "Epoch:11/20 AVG Training Loss:0.903 AVG Test Loss:0.945 AVG Training Acc 65.99 % AVG Test Acc 64.10 %\n",
      "Best Accuracy:65.99343310934637 %\n",
      "Epoch:12/20 AVG Training Loss:0.908 AVG Test Loss:1.008 AVG Training Acc 65.63 % AVG Test Acc 61.93 %\n",
      "Epoch:13/20 AVG Training Loss:0.909 AVG Test Loss:1.009 AVG Training Acc 65.35 % AVG Test Acc 61.09 %\n",
      "Epoch:14/20 AVG Training Loss:0.906 AVG Test Loss:1.040 AVG Training Acc 65.77 % AVG Test Acc 59.99 %\n",
      "Epoch:15/20 AVG Training Loss:0.904 AVG Test Loss:0.997 AVG Training Acc 65.69 % AVG Test Acc 62.15 %\n",
      "Epoch:16/20 AVG Training Loss:0.906 AVG Test Loss:0.912 AVG Training Acc 65.64 % AVG Test Acc 65.26 %\n",
      "Epoch:17/20 AVG Training Loss:0.905 AVG Test Loss:0.968 AVG Training Acc 65.77 % AVG Test Acc 63.19 %\n",
      "Epoch:18/20 AVG Training Loss:0.900 AVG Test Loss:0.977 AVG Training Acc 65.67 % AVG Test Acc 62.81 %\n",
      "Epoch:19/20 AVG Training Loss:0.903 AVG Test Loss:0.933 AVG Training Acc 65.71 % AVG Test Acc 64.22 %\n",
      "Epoch:20/20 AVG Training Loss:0.905 AVG Test Loss:0.957 AVG Training Acc 65.69 % AVG Test Acc 64.10 %\n",
      "Fold 4\n",
      "Epoch:1/20 AVG Training Loss:0.908 AVG Test Loss:0.938 AVG Training Acc 65.40 % AVG Test Acc 64.13 %\n",
      "Epoch:2/20 AVG Training Loss:0.909 AVG Test Loss:1.043 AVG Training Acc 65.56 % AVG Test Acc 59.68 %\n",
      "Epoch:3/20 AVG Training Loss:0.907 AVG Test Loss:0.907 AVG Training Acc 65.53 % AVG Test Acc 65.40 %\n",
      "Epoch:4/20 AVG Training Loss:0.907 AVG Test Loss:0.951 AVG Training Acc 65.49 % AVG Test Acc 63.53 %\n",
      "Epoch:5/20 AVG Training Loss:0.905 AVG Test Loss:0.952 AVG Training Acc 65.62 % AVG Test Acc 63.44 %\n",
      "Epoch:6/20 AVG Training Loss:0.907 AVG Test Loss:0.955 AVG Training Acc 65.79 % AVG Test Acc 64.04 %\n",
      "Epoch:7/20 AVG Training Loss:0.905 AVG Test Loss:0.923 AVG Training Acc 65.68 % AVG Test Acc 64.59 %\n",
      "Epoch:8/20 AVG Training Loss:0.904 AVG Test Loss:0.947 AVG Training Acc 65.79 % AVG Test Acc 64.82 %\n",
      "Epoch:9/20 AVG Training Loss:0.902 AVG Test Loss:0.928 AVG Training Acc 65.82 % AVG Test Acc 64.32 %\n",
      "Epoch:10/20 AVG Training Loss:0.903 AVG Test Loss:0.936 AVG Training Acc 65.73 % AVG Test Acc 65.16 %\n",
      "Epoch:11/20 AVG Training Loss:0.902 AVG Test Loss:0.914 AVG Training Acc 65.79 % AVG Test Acc 65.64 %\n",
      "Epoch:12/20 AVG Training Loss:0.901 AVG Test Loss:0.908 AVG Training Acc 65.76 % AVG Test Acc 65.33 %\n",
      "Epoch:13/20 AVG Training Loss:0.899 AVG Test Loss:1.080 AVG Training Acc 65.77 % AVG Test Acc 59.78 %\n",
      "Epoch:14/20 AVG Training Loss:0.900 AVG Test Loss:1.080 AVG Training Acc 65.82 % AVG Test Acc 58.99 %\n",
      "Epoch:15/20 AVG Training Loss:0.902 AVG Test Loss:0.946 AVG Training Acc 66.07 % AVG Test Acc 63.82 %\n",
      "Best Accuracy:66.06788332315212 %\n",
      "Epoch:16/20 AVG Training Loss:0.902 AVG Test Loss:0.940 AVG Training Acc 65.94 % AVG Test Acc 64.52 %\n",
      "Epoch:17/20 AVG Training Loss:0.902 AVG Test Loss:0.994 AVG Training Acc 65.71 % AVG Test Acc 61.16 %\n",
      "Epoch:18/20 AVG Training Loss:0.902 AVG Test Loss:0.948 AVG Training Acc 66.09 % AVG Test Acc 63.46 %\n",
      "Best Accuracy:66.09460904092853 %\n",
      "Epoch:19/20 AVG Training Loss:0.905 AVG Test Loss:0.900 AVG Training Acc 65.49 % AVG Test Acc 66.04 %\n",
      "Epoch:20/20 AVG Training Loss:0.901 AVG Test Loss:0.923 AVG Training Acc 65.61 % AVG Test Acc 64.61 %\n",
      "Fold 5\n",
      "Epoch:1/20 AVG Training Loss:0.902 AVG Test Loss:0.922 AVG Training Acc 65.93 % AVG Test Acc 65.11 %\n",
      "Epoch:2/20 AVG Training Loss:0.903 AVG Test Loss:0.921 AVG Training Acc 65.67 % AVG Test Acc 65.37 %\n",
      "Epoch:3/20 AVG Training Loss:0.901 AVG Test Loss:0.989 AVG Training Acc 65.67 % AVG Test Acc 61.90 %\n",
      "Epoch:4/20 AVG Training Loss:0.902 AVG Test Loss:0.953 AVG Training Acc 65.75 % AVG Test Acc 64.83 %\n",
      "Epoch:5/20 AVG Training Loss:0.898 AVG Test Loss:0.950 AVG Training Acc 65.95 % AVG Test Acc 64.44 %\n",
      "Epoch:6/20 AVG Training Loss:0.897 AVG Test Loss:0.934 AVG Training Acc 65.98 % AVG Test Acc 65.09 %\n",
      "Epoch:7/20 AVG Training Loss:0.898 AVG Test Loss:0.972 AVG Training Acc 65.89 % AVG Test Acc 63.15 %\n",
      "Epoch:8/20 AVG Training Loss:0.896 AVG Test Loss:0.970 AVG Training Acc 66.08 % AVG Test Acc 63.05 %\n",
      "Epoch:9/20 AVG Training Loss:0.897 AVG Test Loss:0.988 AVG Training Acc 66.08 % AVG Test Acc 62.45 %\n",
      "Epoch:10/20 AVG Training Loss:0.893 AVG Test Loss:0.957 AVG Training Acc 66.01 % AVG Test Acc 63.44 %\n",
      "Epoch:11/20 AVG Training Loss:0.897 AVG Test Loss:1.001 AVG Training Acc 65.92 % AVG Test Acc 62.46 %\n",
      "Epoch:12/20 AVG Training Loss:0.895 AVG Test Loss:0.951 AVG Training Acc 66.12 % AVG Test Acc 64.49 %\n",
      "Best Accuracy:66.12324373854612 %\n",
      "Epoch:13/20 AVG Training Loss:0.902 AVG Test Loss:0.961 AVG Training Acc 65.73 % AVG Test Acc 64.42 %\n",
      "Epoch:14/20 AVG Training Loss:0.902 AVG Test Loss:0.958 AVG Training Acc 65.83 % AVG Test Acc 63.80 %\n",
      "Epoch:15/20 AVG Training Loss:0.897 AVG Test Loss:0.986 AVG Training Acc 65.79 % AVG Test Acc 62.76 %\n",
      "Epoch:16/20 AVG Training Loss:0.895 AVG Test Loss:0.960 AVG Training Acc 66.09 % AVG Test Acc 63.75 %\n",
      "Epoch:17/20 AVG Training Loss:0.900 AVG Test Loss:0.995 AVG Training Acc 65.77 % AVG Test Acc 62.46 %\n",
      "Epoch:18/20 AVG Training Loss:0.897 AVG Test Loss:1.024 AVG Training Acc 65.97 % AVG Test Acc 61.12 %\n",
      "Epoch:19/20 AVG Training Loss:0.898 AVG Test Loss:1.009 AVG Training Acc 65.87 % AVG Test Acc 62.34 %\n",
      "Epoch:20/20 AVG Training Loss:0.897 AVG Test Loss:0.941 AVG Training Acc 65.90 % AVG Test Acc 64.78 %\n",
      "Fold 6\n",
      "Epoch:1/20 AVG Training Loss:0.907 AVG Test Loss:0.904 AVG Training Acc 65.62 % AVG Test Acc 65.82 %\n",
      "Epoch:2/20 AVG Training Loss:0.906 AVG Test Loss:0.947 AVG Training Acc 65.48 % AVG Test Acc 63.26 %\n",
      "Epoch:3/20 AVG Training Loss:0.907 AVG Test Loss:0.918 AVG Training Acc 65.59 % AVG Test Acc 64.97 %\n",
      "Epoch:4/20 AVG Training Loss:0.902 AVG Test Loss:0.913 AVG Training Acc 65.69 % AVG Test Acc 65.07 %\n",
      "Epoch:5/20 AVG Training Loss:0.903 AVG Test Loss:0.973 AVG Training Acc 65.61 % AVG Test Acc 62.46 %\n",
      "Epoch:6/20 AVG Training Loss:0.904 AVG Test Loss:0.935 AVG Training Acc 65.72 % AVG Test Acc 64.35 %\n",
      "Epoch:7/20 AVG Training Loss:0.901 AVG Test Loss:0.960 AVG Training Acc 65.81 % AVG Test Acc 62.78 %\n",
      "Epoch:8/20 AVG Training Loss:0.903 AVG Test Loss:0.932 AVG Training Acc 65.90 % AVG Test Acc 64.55 %\n",
      "Epoch:9/20 AVG Training Loss:0.897 AVG Test Loss:0.924 AVG Training Acc 66.04 % AVG Test Acc 64.64 %\n",
      "Epoch:10/20 AVG Training Loss:0.898 AVG Test Loss:0.929 AVG Training Acc 65.91 % AVG Test Acc 64.00 %\n",
      "Epoch:11/20 AVG Training Loss:0.896 AVG Test Loss:0.921 AVG Training Acc 65.92 % AVG Test Acc 64.73 %\n",
      "Epoch:12/20 AVG Training Loss:0.898 AVG Test Loss:0.922 AVG Training Acc 66.01 % AVG Test Acc 64.78 %\n",
      "Epoch:13/20 AVG Training Loss:0.897 AVG Test Loss:1.014 AVG Training Acc 65.94 % AVG Test Acc 62.25 %\n",
      "Epoch:14/20 AVG Training Loss:0.901 AVG Test Loss:0.951 AVG Training Acc 65.99 % AVG Test Acc 63.99 %\n",
      "Epoch:15/20 AVG Training Loss:0.897 AVG Test Loss:0.964 AVG Training Acc 66.01 % AVG Test Acc 63.14 %\n",
      "Epoch:16/20 AVG Training Loss:0.898 AVG Test Loss:1.024 AVG Training Acc 66.04 % AVG Test Acc 60.48 %\n",
      "Epoch:17/20 AVG Training Loss:0.898 AVG Test Loss:0.959 AVG Training Acc 66.00 % AVG Test Acc 63.21 %\n",
      "Epoch:18/20 AVG Training Loss:0.898 AVG Test Loss:0.905 AVG Training Acc 66.10 % AVG Test Acc 65.52 %\n",
      "Epoch:19/20 AVG Training Loss:0.898 AVG Test Loss:0.927 AVG Training Acc 65.98 % AVG Test Acc 65.45 %\n",
      "Epoch:20/20 AVG Training Loss:0.897 AVG Test Loss:0.966 AVG Training Acc 66.07 % AVG Test Acc 62.68 %\n",
      "Fold 7\n",
      "Epoch:1/20 AVG Training Loss:0.905 AVG Test Loss:0.876 AVG Training Acc 65.67 % AVG Test Acc 66.22 %\n",
      "Epoch:2/20 AVG Training Loss:0.904 AVG Test Loss:0.901 AVG Training Acc 65.73 % AVG Test Acc 66.15 %\n",
      "Epoch:3/20 AVG Training Loss:0.905 AVG Test Loss:0.894 AVG Training Acc 65.52 % AVG Test Acc 66.01 %\n",
      "Epoch:4/20 AVG Training Loss:0.903 AVG Test Loss:0.903 AVG Training Acc 65.76 % AVG Test Acc 65.65 %\n",
      "Epoch:5/20 AVG Training Loss:0.903 AVG Test Loss:0.915 AVG Training Acc 65.71 % AVG Test Acc 65.48 %\n",
      "Epoch:6/20 AVG Training Loss:0.899 AVG Test Loss:0.937 AVG Training Acc 65.84 % AVG Test Acc 63.45 %\n",
      "Epoch:7/20 AVG Training Loss:0.902 AVG Test Loss:0.941 AVG Training Acc 65.62 % AVG Test Acc 63.47 %\n",
      "Epoch:8/20 AVG Training Loss:0.898 AVG Test Loss:0.916 AVG Training Acc 65.97 % AVG Test Acc 64.81 %\n",
      "Epoch:9/20 AVG Training Loss:0.902 AVG Test Loss:0.883 AVG Training Acc 65.82 % AVG Test Acc 66.24 %\n",
      "Epoch:10/20 AVG Training Loss:0.901 AVG Test Loss:0.944 AVG Training Acc 65.94 % AVG Test Acc 63.33 %\n",
      "Epoch:11/20 AVG Training Loss:0.902 AVG Test Loss:0.959 AVG Training Acc 65.82 % AVG Test Acc 63.63 %\n",
      "Epoch:12/20 AVG Training Loss:0.901 AVG Test Loss:0.886 AVG Training Acc 65.81 % AVG Test Acc 66.44 %\n",
      "Epoch:13/20 AVG Training Loss:0.897 AVG Test Loss:0.880 AVG Training Acc 65.97 % AVG Test Acc 66.60 %\n",
      "Epoch:14/20 AVG Training Loss:0.898 AVG Test Loss:0.932 AVG Training Acc 65.93 % AVG Test Acc 65.38 %\n",
      "Epoch:15/20 AVG Training Loss:0.901 AVG Test Loss:0.925 AVG Training Acc 65.93 % AVG Test Acc 64.91 %\n",
      "Epoch:16/20 AVG Training Loss:0.900 AVG Test Loss:0.883 AVG Training Acc 66.03 % AVG Test Acc 66.68 %\n",
      "Epoch:17/20 AVG Training Loss:0.900 AVG Test Loss:0.899 AVG Training Acc 65.96 % AVG Test Acc 64.59 %\n",
      "Epoch:18/20 AVG Training Loss:0.896 AVG Test Loss:0.915 AVG Training Acc 66.14 % AVG Test Acc 65.60 %\n",
      "Best Accuracy:66.13916197384746 %\n",
      "Epoch:19/20 AVG Training Loss:0.898 AVG Test Loss:0.896 AVG Training Acc 65.91 % AVG Test Acc 65.77 %\n",
      "Epoch:20/20 AVG Training Loss:0.898 AVG Test Loss:0.924 AVG Training Acc 66.08 % AVG Test Acc 65.27 %\n",
      "Fold 8\n",
      "Epoch:1/20 AVG Training Loss:0.906 AVG Test Loss:0.910 AVG Training Acc 65.52 % AVG Test Acc 66.24 %\n",
      "Epoch:2/20 AVG Training Loss:0.901 AVG Test Loss:0.944 AVG Training Acc 65.88 % AVG Test Acc 64.79 %\n",
      "Epoch:3/20 AVG Training Loss:0.899 AVG Test Loss:0.891 AVG Training Acc 65.98 % AVG Test Acc 66.62 %\n",
      "Epoch:4/20 AVG Training Loss:0.899 AVG Test Loss:0.892 AVG Training Acc 65.99 % AVG Test Acc 65.84 %\n",
      "Epoch:5/20 AVG Training Loss:0.896 AVG Test Loss:0.888 AVG Training Acc 65.97 % AVG Test Acc 65.50 %\n",
      "Epoch:6/20 AVG Training Loss:0.895 AVG Test Loss:0.893 AVG Training Acc 65.99 % AVG Test Acc 66.56 %\n",
      "Epoch:7/20 AVG Training Loss:0.899 AVG Test Loss:0.932 AVG Training Acc 65.87 % AVG Test Acc 64.55 %\n",
      "Epoch:8/20 AVG Training Loss:0.899 AVG Test Loss:0.906 AVG Training Acc 65.93 % AVG Test Acc 65.74 %\n",
      "Epoch:9/20 AVG Training Loss:0.900 AVG Test Loss:1.021 AVG Training Acc 65.63 % AVG Test Acc 60.74 %\n",
      "Epoch:10/20 AVG Training Loss:0.897 AVG Test Loss:0.900 AVG Training Acc 66.00 % AVG Test Acc 66.07 %\n",
      "Epoch:11/20 AVG Training Loss:0.896 AVG Test Loss:0.908 AVG Training Acc 66.10 % AVG Test Acc 65.48 %\n",
      "Epoch:12/20 AVG Training Loss:0.900 AVG Test Loss:0.927 AVG Training Acc 65.99 % AVG Test Acc 63.97 %\n",
      "Epoch:13/20 AVG Training Loss:0.897 AVG Test Loss:0.962 AVG Training Acc 66.04 % AVG Test Acc 63.11 %\n",
      "Epoch:14/20 AVG Training Loss:0.898 AVG Test Loss:0.942 AVG Training Acc 65.97 % AVG Test Acc 64.93 %\n",
      "Epoch:15/20 AVG Training Loss:0.897 AVG Test Loss:0.974 AVG Training Acc 66.11 % AVG Test Acc 63.28 %\n",
      "Epoch:16/20 AVG Training Loss:0.901 AVG Test Loss:1.058 AVG Training Acc 65.89 % AVG Test Acc 59.19 %\n",
      "Epoch:17/20 AVG Training Loss:0.897 AVG Test Loss:0.960 AVG Training Acc 66.01 % AVG Test Acc 63.71 %\n",
      "Epoch:18/20 AVG Training Loss:0.897 AVG Test Loss:0.926 AVG Training Acc 65.91 % AVG Test Acc 64.93 %\n",
      "Epoch:19/20 AVG Training Loss:0.897 AVG Test Loss:0.930 AVG Training Acc 65.87 % AVG Test Acc 64.18 %\n",
      "Epoch:20/20 AVG Training Loss:0.900 AVG Test Loss:0.993 AVG Training Acc 65.92 % AVG Test Acc 62.13 %\n",
      "Fold 9\n",
      "Epoch:1/20 AVG Training Loss:0.900 AVG Test Loss:0.942 AVG Training Acc 65.84 % AVG Test Acc 63.57 %\n",
      "Epoch:2/20 AVG Training Loss:0.896 AVG Test Loss:0.964 AVG Training Acc 66.03 % AVG Test Acc 63.57 %\n",
      "Epoch:3/20 AVG Training Loss:0.896 AVG Test Loss:0.925 AVG Training Acc 65.93 % AVG Test Acc 65.19 %\n",
      "Epoch:4/20 AVG Training Loss:0.894 AVG Test Loss:0.957 AVG Training Acc 66.20 % AVG Test Acc 63.59 %\n",
      "Best Accuracy:66.20215710604181 %\n",
      "Epoch:5/20 AVG Training Loss:0.895 AVG Test Loss:0.946 AVG Training Acc 66.14 % AVG Test Acc 64.11 %\n",
      "Epoch:6/20 AVG Training Loss:0.897 AVG Test Loss:0.916 AVG Training Acc 66.07 % AVG Test Acc 65.14 %\n",
      "Epoch:7/20 AVG Training Loss:0.896 AVG Test Loss:0.916 AVG Training Acc 65.90 % AVG Test Acc 66.05 %\n",
      "Epoch:8/20 AVG Training Loss:0.894 AVG Test Loss:0.944 AVG Training Acc 66.21 % AVG Test Acc 64.05 %\n",
      "Best Accuracy:66.21361076644078 %\n",
      "Epoch:9/20 AVG Training Loss:0.898 AVG Test Loss:0.943 AVG Training Acc 66.04 % AVG Test Acc 64.26 %\n",
      "Epoch:10/20 AVG Training Loss:0.893 AVG Test Loss:0.949 AVG Training Acc 66.13 % AVG Test Acc 64.11 %\n",
      "Epoch:11/20 AVG Training Loss:0.895 AVG Test Loss:0.945 AVG Training Acc 66.22 % AVG Test Acc 64.36 %\n",
      "Best Accuracy:66.21551970984059 %\n",
      "Epoch:12/20 AVG Training Loss:0.896 AVG Test Loss:0.917 AVG Training Acc 66.08 % AVG Test Acc 65.55 %\n",
      "Epoch:13/20 AVG Training Loss:0.896 AVG Test Loss:0.950 AVG Training Acc 66.21 % AVG Test Acc 65.33 %\n",
      "Epoch:14/20 AVG Training Loss:0.896 AVG Test Loss:0.940 AVG Training Acc 65.88 % AVG Test Acc 65.22 %\n",
      "Epoch:15/20 AVG Training Loss:0.894 AVG Test Loss:0.902 AVG Training Acc 66.00 % AVG Test Acc 65.95 %\n",
      "Epoch:16/20 AVG Training Loss:0.893 AVG Test Loss:0.963 AVG Training Acc 66.17 % AVG Test Acc 63.66 %\n",
      "Epoch:17/20 AVG Training Loss:0.896 AVG Test Loss:0.907 AVG Training Acc 66.12 % AVG Test Acc 65.46 %\n",
      "Epoch:18/20 AVG Training Loss:0.894 AVG Test Loss:0.922 AVG Training Acc 66.08 % AVG Test Acc 65.14 %\n",
      "Epoch:19/20 AVG Training Loss:0.895 AVG Test Loss:0.925 AVG Training Acc 66.21 % AVG Test Acc 65.09 %\n",
      "Epoch:20/20 AVG Training Loss:0.895 AVG Test Loss:0.985 AVG Training Acc 66.22 % AVG Test Acc 62.96 %\n",
      "Best Accuracy:66.22124654004008 %\n",
      "Fold 10\n",
      "Epoch:1/20 AVG Training Loss:0.897 AVG Test Loss:0.901 AVG Training Acc 66.14 % AVG Test Acc 66.53 %\n",
      "Epoch:2/20 AVG Training Loss:0.897 AVG Test Loss:0.891 AVG Training Acc 66.06 % AVG Test Acc 66.05 %\n",
      "Epoch:3/20 AVG Training Loss:0.896 AVG Test Loss:0.973 AVG Training Acc 66.15 % AVG Test Acc 62.63 %\n",
      "Epoch:4/20 AVG Training Loss:0.892 AVG Test Loss:0.913 AVG Training Acc 66.23 % AVG Test Acc 65.62 %\n",
      "Best Accuracy:66.22506442683974 %\n",
      "Epoch:5/20 AVG Training Loss:0.901 AVG Test Loss:0.922 AVG Training Acc 65.72 % AVG Test Acc 65.48 %\n",
      "Epoch:6/20 AVG Training Loss:0.894 AVG Test Loss:0.946 AVG Training Acc 66.29 % AVG Test Acc 64.23 %\n",
      "Best Accuracy:66.28996850243391 %\n",
      "Epoch:7/20 AVG Training Loss:0.895 AVG Test Loss:0.956 AVG Training Acc 66.17 % AVG Test Acc 63.83 %\n",
      "Epoch:8/20 AVG Training Loss:0.893 AVG Test Loss:0.970 AVG Training Acc 66.13 % AVG Test Acc 62.08 %\n",
      "Epoch:9/20 AVG Training Loss:0.894 AVG Test Loss:0.962 AVG Training Acc 66.09 % AVG Test Acc 63.63 %\n",
      "Epoch:10/20 AVG Training Loss:0.895 AVG Test Loss:0.933 AVG Training Acc 66.13 % AVG Test Acc 65.14 %\n",
      "Epoch:11/20 AVG Training Loss:0.895 AVG Test Loss:0.922 AVG Training Acc 65.96 % AVG Test Acc 64.85 %\n",
      "Epoch:12/20 AVG Training Loss:0.894 AVG Test Loss:1.016 AVG Training Acc 66.23 % AVG Test Acc 61.65 %\n",
      "Epoch:13/20 AVG Training Loss:0.896 AVG Test Loss:0.979 AVG Training Acc 66.11 % AVG Test Acc 62.90 %\n",
      "Epoch:14/20 AVG Training Loss:0.895 AVG Test Loss:0.931 AVG Training Acc 66.04 % AVG Test Acc 64.21 %\n",
      "Epoch:15/20 AVG Training Loss:0.890 AVG Test Loss:0.893 AVG Training Acc 66.37 % AVG Test Acc 66.70 %\n",
      "Best Accuracy:66.37014412522669 %\n",
      "Epoch:16/20 AVG Training Loss:0.891 AVG Test Loss:1.009 AVG Training Acc 66.28 % AVG Test Acc 62.25 %\n",
      "Epoch:17/20 AVG Training Loss:0.893 AVG Test Loss:0.968 AVG Training Acc 66.06 % AVG Test Acc 63.33 %\n",
      "Epoch:18/20 AVG Training Loss:0.895 AVG Test Loss:0.919 AVG Training Acc 66.00 % AVG Test Acc 65.09 %\n",
      "Epoch:19/20 AVG Training Loss:0.891 AVG Test Loss:1.029 AVG Training Acc 66.16 % AVG Test Acc 62.27 %\n",
      "Epoch:20/20 AVG Training Loss:0.890 AVG Test Loss:0.938 AVG Training Acc 66.07 % AVG Test Acc 64.30 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(20)\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((227,227)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder('./color_dataset_2/test', transform=test_transform)\n",
    "test_loaded = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loaded:\n",
    "\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testBatch(model):\n",
    "    model.eval()\n",
    "    # get batch of images from the test DataLoader  \n",
    "    images, labels = next(iter(test_loaded))\n",
    "   \n",
    "    # show all images as one image grid\n",
    "    img = torchvision.utils.make_grid(images)     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "   \n",
    "    # Show the real labels on the screen \n",
    "    print('Real labels: ', ' '.join('%5s' % classes[labels[j]] \n",
    "                               for j in range(batch_size)))\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        \n",
    "        # Let's show the predicted labels on the screen to compare with the real ones\n",
    "        print('Predicted: ', ' '.join('%5s' % classes[predictions[j]] \n",
    "                                for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "path = \"apurated_model_alex.pth\"\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG Test Loss:1.431 AVG Test Acc 57.25 %\n"
     ]
    }
   ],
   "source": [
    "valid_loss,val_correct = test_model(model)\n",
    "\n",
    "valid_loss = valid_loss / len(test_loaded.sampler)\n",
    "val_acc = val_correct / len(test_loaded.sampler) * 100\n",
    "\n",
    "print(\"AVG Test Loss:{:.3f} AVG Test Acc {:.2f} %\".format(valid_loss,val_acc))\n",
    "                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
